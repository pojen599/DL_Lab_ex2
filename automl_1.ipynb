{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will see how we can use Bayesian optimization [[Shahriari et al, 2016]](#4.-References) and Regularized Evolution [[Real et al. 2019]](#4.-References) to optimize the architecture of a CNN on the NAS-Bench-201 tabular benchmark. We will also learn how we can empiricially evaluate NAS methods in practice.\n",
    "\n",
    "Before you start, make sure you installed the following dependencies:\n",
    "- numpy (pip install numpy)\n",
    "- matplotlib (pip install matplotlib)\n",
    "- scipy (pip install scipy)\n",
    "- sklearn (pip install sklearn)\n",
    "- pytorch (pip install torch)\n",
    "- ConfigSpace 0.6.1 (pip install ConfigSpace==0.6.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: To complete the exercise, fill all the code below the TODOs and remove the `raise NotImplementedError` line. After doing this for all of them, run the notebook, save it and upload it in a zip file together with the other files in `AutoML.zip`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid training the architectures we will utilize the NAS-Bench-201 tabular benchmark [[Dong and Yang 2020]](#4.-References). The results are compiled into a database, such that we can simply look up the performance of a architecture configuration instead of training it from scratch. For this exercise we will utilize a custom version of NB201 that loads a pickle file and wraps the function $f: \\{ architecture \\} \\rightarrow \\{ error \\}$ in a class called `NB201Benchmark`.\n",
    "\n",
    "**Some details (optional to read):**\n",
    "\n",
    "NAS-Bench-201 search space contains cell-based neural architectures which represent an architecture\n",
    "as a graph, where each cell consists of 4 nodes and 6 edges and each edge has 5 operation candidates,\n",
    "such as zerorize, skip connection, 1-by-1 convolution, 3-by-3 convolution, and 3-by-3 average\n",
    "pooling, which leads to the total of 15626 unique architectures. The macro skeleton is stacked with\n",
    "one stem cell, three stages of 5 repeated cells each, residual blocks between the stages, and final\n",
    "classification layer consisting of a average pooling layer and a fully connected layer with softmax\n",
    "function. The stem cell consists of a 3-by-3 convolution with 16 output channels followed by a batch\n",
    "normalization layer, each cell with three stages has 16, 32 and 64 output channels, respectively.\n",
    "The intermediate residual blocks have convolution layers with the stride 2 for down-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://xuanyidong.com/resources/paper-icon/ICLR-2020/space.png\" width=750cm/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how we can load the benchmark together with the configuration space.\n",
    "Each architecture configuration is encoded as a `Configuration` object and we can easily convert it to a `numpy` array or a `dictionary`. \n",
    "\n",
    "If we evaluate a architecture configuration, we get the test error (dataset choices: `cifar10`, `cifar100`, `imagenet16`) and the time it had taken to train this configuration. The benchmark contains the measured latency (in *ms*) for doing a forward pass through the network on multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from nb201 import NB201Benchmark\n",
    "\n",
    "b = NB201Benchmark(path=\"./nb201.pkl\", dataset='cifar10')\n",
    "cs = b.get_configuration_space()\n",
    "config = cs.sample_configuration() # samples a configuration uniformly at random\n",
    "\n",
    "print(\"Numpy representation: \", config.get_array())\n",
    "print(\"Dict representation: \", config.get_dictionary())\n",
    "\n",
    "y, cost = b.objective_function(config)\n",
    "print(\"Test error: %f %%\" % y)\n",
    "print(\"Runtime %f s\" % cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a parent `Optimizer` class that we will use to build all other optimizers throughout this exercise. It containts an `_initialization` method, that randomly samples and evaluates `{n_init}` configurations, and a `run` method, that runs the search. `evaluate_and_log` evaluates `config` and updates the book keeping attributes of the `Optimizer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, benchmark: ConfigSpace.ConfigurationSpace, n_iters: int=0, n_init: int=0) -> None:\n",
    "        self.benchmark = benchmark\n",
    "        self.cs = benchmark.get_configuration_space()\n",
    "        self.n_iters = n_iters\n",
    "        self.n_init = n_init\n",
    "        \n",
    "        # book keeping\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.runtime = 0\n",
    "        self.total_time = [] #total time passed at every evaluation\n",
    "\n",
    "        # NOTE: incumbent == best configuration we have seen so far\n",
    "        self.incumbent_trajectory = []\n",
    "        self.incumbent_trajectory_error = []\n",
    "        self.incumbent_val = np.inf\n",
    "        self.incumbent = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def _initialization(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate_and_log(self, config: ConfigSpace.Configuration) -> None:\n",
    "        # TODO: evaluate objective function at the candidate point\n",
    "        \n",
    "        self.X.append(config)\n",
    "        self.y.append(error)\n",
    "        self.runtime += cost\n",
    "        self.total_time.append(self.runtime)\n",
    "        \n",
    "        # TODO: check whether we improved upon the current incumbent\n",
    "        # and if yes, update the incumbent_val and incumbent\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # updated incumbent trajectory\n",
    "        self.incumbent_trajectory.append(self.incumbent)\n",
    "        self.incumbent_trajectory_error.append(self.incumbent_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will first implement random search [[Bergstra et al, 2012]](#4.-References), which is, besides its simplicity, usually a quite tough baseline to beat. If you develop a new method, you should always first compare against random search to see whether it actually works or if there is still a bug somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function draws `n_init` hyperparameter configurations uniformly at random from the configuration space and keeps track of the incumbent (i.e the best configuration we have seen so far) after each function evaluation. Since in this exercise we are also interested in comparing different search methods, we update the incumbent after each time steps together with its test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch(Optimizer):\n",
    "    \n",
    "    def _initialization(self) -> None:\n",
    "        # start the random search loop\n",
    "        for i in range(self.n_init):\n",
    "            \n",
    "            # TODO: sample hyperparameter configuration\n",
    "            raise NotImplementedError\n",
    "    \n",
    "            # TODO: evaluate and store results\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def run(self) -> None:\n",
    "        self._initialization()\n",
    "\n",
    "opt = RandomSearch(benchmark=b, n_init=100)\n",
    "opt.run()\n",
    "\n",
    "plt.plot(opt.total_time, opt.incumbent_trajectory_error)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"test error\")\n",
    "plt.xlabel(\"wallclock time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we we will implement Bayesian optimization [[Snoek et al, 2012]](#4.-References).\n",
    "As we saw in the lecture, Bayesian optimization has two main ingredients: the probablistic model and the acquisition function. Since we have a discrete space here, we will first use random forest [[Breimann et al, 2001]](#4.-References) to model the objective function instead of Gaussian processes which are the usually used for continuous spaces. For the acquisition function we will use upper confidence bound, probability of improvement and expected improvement, which are probably the most popular one in the literature. Additionaly, we also need an optimizer to maximize the acquisition function, and, due to the discrete space, we cannot use standard optimizer, such as for example `scipy.optimize`. Instead, we will implemented a simple stochastic local search method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the model. We will we write a wrapper around sklearn's random forest module, which returns for a given test point only the mean prediction. However, to compute the acquisition function, we also need the predictive variance. For that, we first loop over the trees to get the individual tree predictions and then compute the mean and variance of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # TODO: Instantiate and fit a random forest on the provided data\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # TODO: Loop over the tree estimators of the random forest and compute the\n",
    "        # mean and variance over their predictions on X_test\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement the probability of improvement and expected improvement acquisition functions. To compute the CDF and the PDF you can use the scipy functions: `scipy.norm.cdf` and `scipy.norm.pdf`. Refer to the following link for more details on these acquisition functions: https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(candidates: np.ndarray, model, y_star: float) -> np.ndarray:\n",
    "    \"\"\" Upper Confidence Bound Acquisition Function \"\"\"\n",
    "    explore_factor = 0.01\n",
    "    mean, var = model.predict(candidates)\n",
    "    sigma = np.sqrt(var)\n",
    "    ucb = mean + explore_factor * sigma\n",
    "    return ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def ei(candidates: np.ndarray, model, y_star: float) -> np.ndarray:\n",
    "    \"\"\" Expected Improvement Acquisition Function \"\"\"\n",
    "    # TODO: compute the improvement for the candidate points over y_star in expectation based on the model's predictions\n",
    "    epsilon = 0.0000001 # add this to sigma in denominator to avoid division by 0\n",
    "    explore_factor = 0.01\n",
    "    raise NotImplementedError\n",
    "\n",
    "def pi(candidates: np.ndarray, model, y_star: float) -> np.ndarray:\n",
    "    \"\"\" Probability of Improvement Acquisition Function \"\"\"\n",
    "    # TODO: compute the probability of model predictions being better than y_star\n",
    "    epsilon = 0.0000001 # add this to sigma in denominator to avoid division by 0\n",
    "    explore_factor = 0.01\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned above, to optimize the acquisition function we will implement a simple local search method, which works as follows:\n",
    " 1. start from an initial point `x_init`\n",
    " 2. loop over all its one-step neighbours and compute the acquisition function values\n",
    " 3. jump to the neighbour with the highest acquisition function value\n",
    " 4. repeat step 2 and 3 until we either reach the maximum number of steps `n_steps` or we don't improve anymore\n",
    " 5. return best found configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConfigSpace.util import get_one_exchange_neighbourhood  # see docstring: https://github.com/automl/ConfigSpace/blob/master/ConfigSpace/util.pyx for more details\n",
    "\n",
    "def local_search(acquisition_function, model, y_star: float,\n",
    "                 x_init: ConfigSpace.Configuration, n_steps: int) -> ConfigSpace.Configuration:\n",
    "    current_best = x_init\n",
    "    current_best_value = acquisition_function(x_init.get_array()[None, :])\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        # TODO: evaluate one-step neighbourhood (hint: use get_one_exchange_neighbourhood function)\n",
    "        raise NotImplementedError\n",
    "\n",
    "        max_ei = current_best_value\n",
    "        best_neighbor = None\n",
    "        for neighbor in neighbors:\n",
    "            # TODO: check whether we improved upon the current best\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # TODO: jump to the next neighbour if we improved.\n",
    "        # In case we converged, stop the local search\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return current_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our ingredients together, and we can finally implement the main Bayesian optimization loop.\n",
    "Before we can fit a model, we need to collect some data first. This is called the initial design.\n",
    "Various different initial design strategies exist, but here we will simply sample `n_init` random points.\n",
    "\n",
    "Note that, like for random search, we want to benchmark Bayesian optimization later and need the performance of the incumbent over time. Make sure that you keep track of the incumbent and check after *each function evaluation* whether we improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class BayesianOptimization(RandomSearch):\n",
    "    def run(self, model, acquisition_function=ei, optimizer=local_search):\n",
    "        self._initialization()\n",
    "\n",
    "        # start main BO loop\n",
    "        for i in range(self.n_init, self.n_iters):\n",
    "    \n",
    "            # TODO: fit model\n",
    "            raise NotImplementedError\n",
    "    \n",
    "            # TODO: optimize acquisition function to get candidate point\n",
    "            raise NotImplementedError\n",
    "    \n",
    "            # TODO: evaluate objective function at the candidate point and update incumbent\n",
    "            raise NotImplementedError\n",
    "\n",
    "opt_ucb = BayesianOptimization(benchmark=b, n_iters=100, n_init=20)\n",
    "opt_ucb.run(model=RandomForest, acquisition_function=ucb)\n",
    "opt_pi = BayesianOptimization(benchmark=b, n_iters=100, n_init=20)\n",
    "opt_pi.run(model=RandomForest, acquisition_function=pi)\n",
    "opt_ei = BayesianOptimization(benchmark=b, n_iters=100, n_init=20)\n",
    "opt:ei.run(model=RandomForest, acquisition_function=ei)\n",
    "\n",
    "plt.plot(opt_ucb.total_time, opt_ucb.incumbent_trajectory_error, label='RF UCB')\n",
    "plt.plot(opt_pi.total_time, opt_pi.incumbent_trajectory_error, label='RF PI')\n",
    "plt.plot(opt_ei.total_time, opt_ei.incumbent_trajectory_error, label='RF EI')\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"test error\")\n",
    "plt.xlabel(\"wallclock time (s)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional model we will try DNGO [[Snoek et al, 2015]](#4.-References), which first fits a neural networks with a linear output layer.\n",
    "After training, it chops off the output layer and uses Bayesian linear regression with the output of the last layer as basis functions.\n",
    "During inference time, it pass the test data through the first layers and then uses the precomputed Basis linear regression terms (m, K) to compute the mean and the variance. For more details have a look in Section 3 in the paper by [[Snoek et al, 2015]](#4.-References)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_inputs, n_units=50):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_units)\n",
    "        self.fc2 = nn.Linear(n_units, n_units)\n",
    "        self.fc3 = nn.Linear(n_units, n_units)\n",
    "        self.out = nn.Linear(n_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "    def basis_funcs(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class DNGO(object):\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, num_epochs: int = 200) -> None:\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = 'cuda'\n",
    "        else:\n",
    "            device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        batch_size = 32\n",
    "        learning_rate = 1e-4\n",
    "\n",
    "        # TODO: instantiate the neural network\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # TODO: implement the training loop. Use the MSE loss and Adam optimizer\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # Hyperparameters for the Bayesian linear regression.\n",
    "        # Note: in the paper they sample \\alpha an \\beta from the marginal log-likelihood\n",
    "        # However, for simplicity, we keep them fix\n",
    "        self.alpha = 1E-3\n",
    "        self.beta = 1000\n",
    "\n",
    "        # TODO: compute the Bayesian linear regression terms\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        # TODO: extract the basis functions\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # TODO: compute the mean and the variance based on the Bayesian linear regression terms\n",
    "        raise NotImplementedError\n",
    "\n",
    "        return mean, variance\n",
    "\n",
    "\n",
    "opt = BayesianOptimization(benchmark=b, n_iters=100, n_init=20)\n",
    "opt.run(model=DNGO, acquisition_function=ei)\n",
    "\n",
    "plt.plot(opt.total_time, opt.incumbent_trajectory_error)\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"test error\")\n",
    "plt.xlabel(\"wallclock time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularized Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will implement Regularized Evolution (RE) from [[Real et al. 2019]](#4.-References). We will again inherit from the `RandomSearch` class since RE initializes the population with randomly sampled architectures. We will then run the evolutionary search, where we need to mutate sampled parent architectures from the population. In the NAS-Bench-201 search space the mutations are just 2, namely `clone` and `op_mutation`. `edge_mutation` does not apply here since in NAS-Bench-201 the cell topology is fixed, and the search is only over the operation types on every edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "class RegularizedEvolution(RandomSearch):\n",
    "\n",
    "    def mutate(self, parent_config: ConfigSpace.Configuration) -> ConfigSpace.Configuration:\n",
    "        mutation_type = random.choice(['clone', 'op_mutation'])\n",
    "        if mutation_type == 'op_mutation':\n",
    "            # TODO: pick one random edge out of the 6 edges in the cell\n",
    "            return NotImplementedError\n",
    "            \n",
    "            # TODO: swap the current operation type at this edge with a different\n",
    "            # one sampled randomly (make sure it is not the same operation) from the operation list.\n",
    "            return NotImplementedError\n",
    "\n",
    "        else:\n",
    "            child_arch = parent_config\n",
    "        return child_arch\n",
    "    \n",
    "    def run(self, sample_size: int=10, remove_oldest: bool=True) -> None:\n",
    "        # initialize the population with random architectures\n",
    "        self._initialization()\n",
    "        population = [(arch, error) for arch, error in zip(self.X, self.y)]\n",
    "\n",
    "        # start main RE loop\n",
    "        for i in range(self.n_init, self.n_iters):\n",
    "            sample = []\n",
    "            # TODO: sample randomly \"sample_size\" architectures from the \n",
    "            # current population and add them to the \"sample\" list.\n",
    "            return NotImplementedError\n",
    "\n",
    "            # TODO: select the parent -- the best model in the sample.\n",
    "            # NOTE: keep in mind that we use error here\n",
    "            return NotImplementedError\n",
    "\n",
    "            # TODO: create the child model, evaluate and log it.\n",
    "            return NotImplementedError\n",
    "\n",
    "            # TODO: append the tuple with the child_arch and its error to the population list\n",
    "            return NotImplementedError\n",
    "\n",
    "            if remove_oldest:\n",
    "                # TODO: remove the oldest architecture\n",
    "                return NotImplementedError\n",
    "            else:\n",
    "                # TODO: remove the worst architecture\n",
    "                return NotImplementedError\n",
    "\n",
    "opt_worst = RegularizedEvolution(benchmark=b, n_iters=100, n_init=20)\n",
    "opt_worst.run(remove_oldest=False)\n",
    "opt_old = RegularizedEvolution(benchmark=b, n_iters=100, n_init=20)\n",
    "opt_old.run(remove_oldest=True)\n",
    "\n",
    "plt.plot(opt_worst.total_time, opt_worst.incumbent_trajectory_error, label='RE (worst)')\n",
    "plt.plot(opt_old.total_time, opt_old.incumbent_trajectory_error, label='RE (oldest)')\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"test error\")\n",
    "plt.xlabel(\"wallclock time (s)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major part in developing new optimizers is the empirical comparison to existing baselines.\n",
    "Even though this is often cumbersome and frustrating, it is key to obtain a better understanding and hence to develop better methods.\n",
    "\n",
    "Since the most algorithms are to a certain degree randomized, we need to perform independent runs of each method with a different intialization in order to draw statistical significant conclusions.\n",
    "In practice, this is often problematic due to the high computational cost of the most NAS problems. With our tabular benchmarks however, we can easiliy afford multiple runs and are only limited by the optimizer's overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we run each methods `10` times for `100` iterations and store the incumbent trajectories of each run. If we have parallel resources available, such as for instance with a compute cluster, it is good practice to parallelize everything as much as possible ot get the most efficient workload. However, since we compare only a few methods for a moderate number of runs and iterations, we can afford it to run them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "n_iters = 100\n",
    "n_methods = 4 # RS, RE (oldest), BO (DNGO + EI), BO (RF + EI)\n",
    "\n",
    "incumbent_error = np.zeros((n_methods, n_runs, n_iters))\n",
    "incumbent = [[0 for y in range(n_runs)] for x in range(n_methods)]\n",
    "\n",
    "# TODO: evaluate each method 'n_runs' times for 'n_iters' iterations and update 'incumbents' and 'incumbent_error'\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice property of the tabular benchmarks is that we know the true global optimum (in terms of test error averaged over the four trials).\n",
    "To estimate an optimzer's quality, a popular metric is the regret: $|y_{incumbent} - y_{\\star}|$ which measures the difference between the performance $y_{\\star}$ of the global optimum and the current incumbent performance $y_{incumbent}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_star = b.get_best_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a robust estimated of an optimzer's performance, we first compute the regret and then plot the incumbent trajectory across all independent runs of each optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the mean and standard of the incumbent_error over all runs\n",
    "# https://en.wikipedia.org/wiki/Standard_error\n",
    "mean_incumbent_error = ...\n",
    "incumbent_se = ...\n",
    "\n",
    "x = np.arange(1, mean_incumbent_error.shape[1] + 1) # n_iters\n",
    "# TODO: plot mean regret for every method together with the standard error (use pyplot.fill_between)\n",
    "raise NotImplementedError\n",
    "\n",
    "plt.ylabel(\"test regret\")\n",
    "plt.xlabel(\"number of function evaluations\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically in NAS, the search is done on a small dataset such as CIFAR-10 and the found cells are transferred to a more expensive dataset such as ImageNet and only evaluated (no search done anymore on ImageNet). Thus, it is also good practice to perform an offline evaluation to compute the performance on ImageNet of all incumbents.\n",
    "The tabular benchmarks allow us to do that efficiently and, as for the ImageNet performance, we can again compute the regret to the true performance (y_star_imagenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_imagenet = NB201Benchmark(path=\"./nb201.pkl\", dataset='imagenet16')\n",
    "_, y_star_imagenet = b_test.get_best_configuration()\n",
    "\n",
    "incumbent_error_imagenet = np.zeros((n_methods, n_runs, n_iters))\n",
    "\n",
    "# TODO: compute test performance for all methods and all runs and then the average test regret +/- standard error on ImageNet\n",
    "# HINT: only need to evaluate what we appended to the \"incumbent\" list\n",
    "raise NotImplementedError\n",
    "\n",
    "mean_incumbent_error_imagenet = ...\n",
    "incumbent_se_imagenet = ...\n",
    "regret_graph_test = mean_incumbent_error_imagenet - y_star_imagenet\n",
    "\n",
    "# TODO: plot mean regret on ImageNet for every method together with the standard error (use pyplot.fill_between)\n",
    "raise NotImplementedError\n",
    "\n",
    "plt.ylabel(\"test regret\")\n",
    "plt.xlabel(\"number of function evaluations\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L. Breimann (2001) *Random Forests*  Machine Learning\n",
    "* J. Bergstra and Y. Bengio (2012) *Random Search for Hyper-Parameter Optimization* Journal of Machine Learning Research \n",
    "* B. Shahriari and K. Swersky and Z. Wang and R. Adams and N. de Freitas (2016), *Taking the Human Out of the Loop: {A} Review of {B}ayesian Optimization* Proceedings of the {IEEE}\n",
    "* J. Snoek and H. Larochelle and R. P. Adams (2012) *Practical {B}ayesian Optimization of Machine Learning Algorithms* Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'12)\n",
    "* J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams (2015) *Scalable {B}ayesian Optimization Using Deep Neural Networks* Proceedings of the 32nd International Conference on Machine Learning (ICML'15)\n",
    "* X. Dong, Y. Yang (2020) *NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search* ICLR 2020\n",
    "* E. Real, A. Aggarwal, Y. Huang, Q. V. Le (2019) *Regularized Evolution for Image Classifier Architecture Search* AAAI 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
